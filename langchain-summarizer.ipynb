{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Langchain PDF finder and summarization\n",
    "\n",
    "All the needed imports\n",
    "\n",
    "```\n",
    "pip install langchain\n",
    "pip install python-dotenv\n",
    "pip install pypdf\n",
    "pip install openai\n",
    "pip install chromadb\n",
    "pip install tiktoken\n",
    "pip install arxiv\n",
    "pip install pymupdf\n",
    "```\n",
    "\n",
    "or you can use the requirements.txt\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ArxivRetriever\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T20:40:14.524596600Z",
     "start_time": "2023-05-24T20:39:32.392590600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the API Key stored in the .env file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:40:14.637254100Z",
     "start_time": "2023-05-24T20:40:14.530035100Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY is None:\n",
    "    print('Error: OPENAI_API_KEY is not set in .env file')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM and Initial PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:40:14.637896900Z",
     "start_time": "2023-05-24T20:40:14.571400700Z"
    }
   },
   "outputs": [],
   "source": [
    "pdfpath = './text_classification_with_llm.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:40:18.800287600Z",
     "start_time": "2023-05-24T20:40:14.584762500Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(pdfpath)\n",
    "doc = pdf_loader.load()\n",
    "documents = [doc]\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read pdf document into chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:40:18.848240700Z",
     "start_time": "2023-05-24T20:40:18.812090800Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_docs(docs):\n",
    "    return RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:40:57.015615500Z",
     "start_time": "2023-05-24T20:40:18.833615700Z"
    }
   },
   "outputs": [],
   "source": [
    "docstore = Chroma.from_documents(split_docs(doc), embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relevant sources from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:51:55.370534Z",
     "start_time": "2023-05-24T20:51:55.137773200Z"
    }
   },
   "outputs": [],
   "source": [
    "src_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:51:58.675047900Z",
     "start_time": "2023-05-24T20:51:57.667913800Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_arxiv_id(text):\n",
    "    matches = re.findall(r\"arXiv:(\\d{1,4}\\.\\d{1,5}(?:v\\d+)?)\", text)\n",
    "    return matches[0] if matches else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:52:07.970644Z",
     "start_time": "2023-05-24T20:52:00.812988500Z"
    }
   },
   "outputs": [],
   "source": [
    "sources = src_chain.run(\n",
    "    \"\"\"give me a list of the most relevant sources for this document.\n",
    "       the list should include the arxiv id (e.g 1605.08386)\"\"\"\n",
    "    )\n",
    "sources = sources.split(',')\n",
    "arxiv_list_1 = list(map(extract_arxiv_id, sources))\n",
    "arxiv_list = list(filter(lambda x: x.strip() != '', arxiv_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[' arXiv:2203.02155',\n ' cs/0506075',\n ' arXiv:2104.06599',\n ' arXiv:1901.02860',\n ' arXiv:2212.10509',\n ' arXiv:1810.04805',\n ' arXiv:2104.08762',\n ' arXiv:1810.04805']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T20:52:09.586608100Z",
     "start_time": "2023-05-24T20:52:09.529886700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "['2203.02155',\n '2104.06599',\n '1901.02860',\n '2212.10509',\n '1810.04805',\n '2104.08762',\n '1810.04805']"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T20:52:41.324327500Z",
     "start_time": "2023-05-24T20:52:41.180258200Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download arxiv resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:41:11.068226800Z",
     "start_time": "2023-05-24T20:41:10.111230500Z"
    }
   },
   "outputs": [],
   "source": [
    "retriever = ArxivRetriever(load_max_docs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:41:21.489328100Z",
     "start_time": "2023-05-24T20:41:11.069018500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Training language models to follow instructions\\nwith human feedback\\nLong Ouyang∗\\nJeff Wu∗\\nXu Jiang∗\\nDiogo Almeida∗\\nCarroll L. Wainwright∗\\nPamela Mishkin∗\\nChong Zhang\\nSandhini Agarwal\\nKatarina Slama\\nAlex Ray\\nJohn Schulman\\nJacob Hilton\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\na user’s intent. For example, large language models can generate outputs that\\nare untruthful, toxic, or simply not helpful to the user. In other words, these\\nmodels are not aligned with their users. In this paper, we show an avenue for\\naligning language models with user intent on a wide range of tasks by ﬁne-tuning\\nwith human feedback. Starting with a set of labeler-written prompts and prompts\\nsubmitted through the OpenAI API, we collect a dataset of labeler demonstrations\\nof the desired model behavior, which we use to ﬁne-tune GPT-3 using supervised\\nlearning. We then collect a dataset of rankings of model outputs, which we use to\\nfurther ﬁne-tune this supervised model using reinforcement learning from human\\nfeedback. We call the resulting models InstructGPT. In human evaluations on\\nour prompt distribution, outputs from the 1.3B parameter InstructGPT model are\\npreferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.\\nMoreover, InstructGPT models show improvements in truthfulness and reductions\\nin toxic output generation while having minimal performance regressions on public\\nNLP datasets. Even though InstructGPT still makes simple mistakes, our results\\nshow that ﬁne-tuning with human feedback is a promising direction for aligning\\nlanguage models with human intent.\\n1\\nIntroduction\\nLarge language models (LMs) can be “prompted” to perform a range of natural language process-\\ning (NLP) tasks, given some examples of the task as input. However, these models often express\\nunintended behaviors such as making up facts, generating biased or toxic text, or simply not following\\nuser instructions (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al.,\\n2021; Tamkin et al., 2021; Gehman et al., 2020). This is because the language modeling objective\\n∗Primary authors. This was a joint project of the OpenAI Alignment team. RL and JL are the team leads.\\nCorresponding author: lowe@openai.com.\\n†Work done while at OpenAI. Current afﬁliations: AA: Anthropic; PC: Alignment Research Center.\\narXiv:2203.02155v1  [cs.CL]  4 Mar 2022\\n1.3B\\n6B\\n175B\\nModel size\\n0.2\\n0.4\\n0.6\\nWin rate against SFT 175B\\nModel\\nPPO-ptx\\nPPO\\nSFT\\nGPT (prompted)\\nGPT\\nFigure 1: Human evaluations of various models on our API prompt distribution, evaluated by how\\noften outputs from each model were preferred to those from the 175B SFT model. Our InstructGPT\\nmodels (PPO-ptx) as well as its variant trained without pretraining mix (PPO) signiﬁcantly outperform\\nthe GPT-3 baselines (GPT, GPT prompted); outputs from our 1.3B PPO-ptx model are preferred to\\nthose from the 175B GPT-3. Error bars throughout the paper are 95% conﬁdence intervals.\\nused for many recent large LMs—predicting the next token on a webpage from the internet—is\\ndifferent from the objective “follow the user’s instructions helpfully and safely” (Radford et al., 2019;\\nBrown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Thus, we say that\\nthe language modeling objective is misaligned. Averting these unintended behaviors is especially\\nimportant for language models that are deployed and used in hundreds of applications.\\nWe make progress on aligning language models by training them to act in accordance with the user’s\\nintention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions\\nand implicit intentions such as staying truthful, and not being biased, toxic, or otherwise harmful.\\nUsing the language of Askell et al. (2021), we want language models to be helpful (they should\\nhelp the use', metadata={'Published': '2022-03-04', 'Title': 'Training language models to follow instructions with human feedback', 'Authors': 'Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe', 'Summary': \"Making language models bigger does not inherently make them better at\\nfollowing a user's intent. For example, large language models can generate\\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\\nwords, these models are not aligned with their users. In this paper, we show an\\navenue for aligning language models with user intent on a wide range of tasks\\nby fine-tuning with human feedback. Starting with a set of labeler-written\\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\\noutputs, which we use to further fine-tune this supervised model using\\nreinforcement learning from human feedback. We call the resulting models\\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\\nimprovements in truthfulness and reductions in toxic output generation while\\nhaving minimal performance regressions on public NLP datasets. Even though\\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\\nhuman feedback is a promising direction for aligning language models with human\\nintent.\"})]\n"
     ]
    }
   ],
   "source": [
    "for src in arxiv_list:\n",
    "    doc = retriever.get_relevant_documents(query=src)\n",
    "    if doc == []:\n",
    "        continue\n",
    "    documents.append(doc)\n",
    "    print(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize retrieved documents with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:41:21.786882200Z",
     "start_time": "2023-05-24T20:41:21.496042100Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a comprehensive summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:41:21.936242100Z",
     "start_time": "2023-05-24T20:41:21.541910500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='Text Classiﬁcation via Large Language Models\\nXiaofei Sun\\x07, Xiaoya Li|, Jiwei Li\\x07;|, Fei Wu\\x07\\nShangwei GuoN, Tianwei Zhangª, Guoyin WangF\\nAbstract\\nDespite the remarkable success of large-\\nscale Language Models (LLMs) such as\\nGPT-3, their performances still signiﬁcantly\\nunderperform ﬁne-tuned models in the task of\\ntext classiﬁcation. This is due to (1) the lack\\nof reasoning ability in addressing complex\\nlinguistic phenomena (e.g., intensiﬁcation,\\ncontrast, irony etc); (2) limited number of\\ntokens allowed in in-context learning.\\nIn this paper, we introduce Clue And\\nReasoning Prompting (CARP). CARP adopts\\na progressive reasoning strategy tailored to\\naddressing the complex linguistic phenomena\\ninvolved in text classiﬁcation: CARP ﬁrst\\nprompts LLMs to ﬁnd superﬁcial clues\\n(e.g., keywords, tones, semantic relations,\\nreferences, etc), based on which a diagnostic\\nreasoning process is induced for ﬁnal\\ndecisions. To further address the limited-\\ntoken issue, CARP uses a ﬁne-tuned model on\\nthe supervised dataset for kNN demonstration\\nsearch in the in-context learning, allowing the\\nmodel to take the advantage of both LLM’s\\ngeneralization ability and the task-speciﬁc\\nevidence provided by the full labeled dataset.\\nRemarkably, CARP yields new SOTA\\nperformances on 4 out of 5 widely-used\\ntext-classiﬁcation benchmarks, 97.39 (+1.24)\\non SST-2, 96.40 (+0.72) on AGNews, 98.78\\n(+0.25) on R8 and 96.95 (+0.6) on R52, and\\na performance comparable to SOTA on MR\\n(92.39 v.s. 93.3). More importantly, we ﬁnd\\nthat CARP delivers impressive abilities on\\nlow-resource and domain-adaptation setups.\\nSpeciﬁcally, Speciﬁcally, using 16 examples\\nper class, CARP achieves comparable\\nperformances to supervised models with\\n1,024 examples per class.1\\n1\\x07Zhejiang University,|Shannon.AI,FAmazon\\nªNanyang Technological University,NChongqing University\\n{xiaofei_sun, wufei, jiwei_li}@zju.edu.cn\\nxiaoya_li@shannonai.com, swguo@cqu.edu.cn\\ntianwei.zhang@ntu.edu.sg, guoyiwan@amazon.com1 Introduction\\nLarge language models (LLMs) (Radford et al.,\\n2019a; Xue et al., 2020; Zhang et al., 2022a; Rae\\net al., 2021; Brown et al., 2020; Chowdhery et al.,\\n2022; Ouyang et al., 2022; Thoppilan et al., 2022)\\nhave shown the ability for in-context learning (ICL).\\nGiven a few demonstration examples, LLMs are\\nprompted to generate results for a new test example,\\nand have achieved performance comparable to\\nsupervised baselines or even state-of-the-art results\\nin a variety of natural language processing (NLP)\\ntasks such as question answering (Trivedi et al.,\\n2022), natural language inference, (Schick and\\nSchütze, 2020), named entity recognition (Wang\\net al., 2023), relation extraction (Wan et al., 2023)\\nand information extraction (Han et al., 2021).\\nIn spite of the success, LLMs with ICL\\nstill signiﬁcantly underperform ﬁne-tuned models\\nfor text classiﬁcation. This is due to two\\nreasons: (1) Text classiﬁcation requires models\\nwith more powerful reasoning abilities to\\nresolve complex linguistic phenomenon including\\nclause composition (e.g., concession, negation,\\nintensiﬁcation), irony, etc. Recent efforts to\\nimprove LLMs’ reasoning capabilities (Wei et al.,\\n2022b; Kojima et al., 2022; Ye and Durrett, 2022;\\nZhang et al., 2022b) mainly focus on tackling\\nmath problems, and thus are not tailored to\\naddressing the reasoning process necessary for the\\nmultitude of intricate linguistic phenomena in text\\nclassiﬁcation; (2) This number of demonstration\\nexamples allowed in in-context learning is limited,\\ne.g., the longest context allowed for GPT-3 is 4,096\\nsubtokens. Therefore, LLMs are only able to take\\nthe advantage of a small proportion of the training\\nset, performing well below supervised baselines;\\nIn this paper, we introduce ClueAndReasoning\\nPrompting (CARP), an extensible, annotation-\\nfree and efﬁcient framework for text classiﬁcation\\nvia large language models. To address thearXiv:2305.08377v1  [cs.CL]  15 May 2023', metadata={'source': './text_classification_with_llm.pdf', 'page': 0}),\n Document(page_content='This is an overall sentiment classifier for movie reviews.First, list CLUES (i.e., keywords, phrases, contextual information, semantic relations, semantic meaning, tones, references) that support the sentiment determination of input..Second, deduce the diagnostic REASONING process from premises (i.e., clues, input) that supports the INPUT sentiment determination (Limit the number of words to 130).Third, based on clues, reasoning and input, determine the overall SENTIMENT of INPUT as Positive or Negative.INPUT: press the delete keyCLUES: delete keyREASONING: The phrase \"delete key\" implies an action of removing something, which could be interpreted as a negative sentiment.SENTIMENT: NegativeThis is an overall sentiment classifier for movie reviews. Classify the overall SENTIMENT of the INPUT as Positive or Negative. INPUT: press the delete keySENTIMENT: Let\\'s think step-by-step. The input does not contain any words that would indicate a sentiment, so it is not possible to classify the sentiment as either positive or negative.This is an overall sentiment classifier for movie reviews. Classify the overall SENTIMENT of the INPUT as Positive or Negative.INPUT: press the delete keySENTIMENT: Neutral\\n(b)\\n(c)(a)Figure 1: Examples of zero-shot prompting methods for the text classiﬁcation task: (a)represents for the vanilla\\nprompting method; (b)denotes for the Chain-of-Thought (CoT) (Kojima et al., 2022) prompting method; c\\nrepresents for the proposed CARP prompting method.\\nreasoning process necessary for handling the\\nlinguistic phenomena in text classiﬁcation, CARP\\ndecomposes the reasoning process into three steps,\\nwhere LLMs are ﬁrst prompted to ﬁnd superﬁcial\\nclues (e.g., keywords, tones, semantic relations,\\netc) in the given text; next, CARP treats the clues\\nand input as premises and induce a diagnostic\\nreasoning process; and ﬁnally determine the ﬁnal\\nlabel considering the above two steps. We ﬁnd\\nthis progressive reasoning strategy to be effective\\nin enhancing LLMs’ ability in language reasoning\\ninvolved in text classiﬁcation. Due to the limited\\nnumber of tokens allowed in context, a more\\neffective demonstration search is needed. CARP\\nuses a ﬁne-tuned model on the supervised dataset\\nforkNN demonstration search for in-context\\nlearning. Since the ﬁne-tuned model is trained\\nbased on task-speciﬁc labels, it guarantees that\\nretrieved samples are close to the input sequence\\nwith respect to the task. Using ﬁne-tuned models\\nfor demonstration search provides a channel to\\nconnect LLMs with the full training set, in\\nspite of the limited number of tokens allowed in\\ndemonstrations. This strategy lets the model take\\nthe advantage of both the LLMs’ generalization\\nabilities and all task-speciﬁc evidence provided by\\nthe training dataset.Remarkably, CARP yields new SOTA\\nperformances on four out of 5 widely-used\\ntext-classiﬁcation benchmarks, 97.39 (+1.24)\\non SST-2, 96.40 (+0.72) on AGNews, 98.78\\n(+0.25) on R8 and 96.95 (+0.6) on R52, and\\na performance comparable to SOTA on MR\\n(92.39 v.s. 93.3). More importantly, we ﬁnd that\\nCARP delivers impressive ability on low-resource\\nand domain adaptation setups with orders of\\nmagnitude fewer training examples. Speciﬁcally,\\nCARP achieves comparable performances with 16\\nexamples per class to supervised models trained\\non the full training set containing more than 1\\nthousand examples per class. This demonstrates\\nthe capabilities of CARP in real-world text\\nclassiﬁcation cases where training data is limited.\\n2 Related Work\\n2.1 Large Language Models\\nLarge language models (LLMs) are models that\\nare trained using self-teaching algorithms on large\\nunlabeled corpora. With emergent capabilities (Xie\\net al., 2021; Wei et al., 2022a), LLMs achieve\\nsigniﬁcant performance boosts in NLP tasks.\\nLLMs can be broadly divided into three\\ncategories based on the model architecture. The', metadata={'source': './text_classification_with_llm.pdf', 'page': 1}),\n Document(page_content='This is an overall sentiment classifier for movie reviews. Classify the overall SENTIMENT of the INPUT as Positive or Negative. INPUT: noycefilms it more as a shocking history lesson than as drama.SENTIMENT: Let\\'s think step-by-step. The input states that the films are \"more\" something than something else. This implies that the films are at least somewhat of the first thing mentioned. In this case, the films are more of a \"shocking history lesson\" than \"drama\". This implies that the sentiment is positive, as the films are more of a history lesson than drama. The overall sentiment is Positive.INPUT: press the delete keySENTIMENT: Let\\'s think step-by-step. The input does not provide any context or opinion, so it is impossible to determine the sentiment. Therefore, the overall sentiment is Neutral.This is an overall sentiment classifier for movie reviews. Classify the overall SENTIMENT of the INPUT as Positive or Negative. INPUT: noycefilms it more as a shocking history lesson than as drama.SENTIMENT: PositiveINPUT: press the delete keySENTIMENT: Neutral\\n(a)\\n(b)\\nThis is an overall sentiment classifier for movie reviews.First, present CLUES (i.e., keywords, phrases, contextual information, semantic relations, semantic meaning, tones, references) that support the sentiment determination of input.Second, deduce a diagnostic REASONING process from premises (i.e., clues, input) that supports the sentiment determination (Limit the number of words to 130).Third, determine theoverall SENTIMENT of INPUT as Positive or Negative considering CLUES, the REASONING process and the INPUT.INPUT: noycefilms it more as a shocking history lesson than as drama.CLUES: -Positive: \"more,\" \"shocking,\" \"history lesson,\" \"drama.\" -Negative: None.REASONING: The input contains positive words such as \"more,\" \"shocking,\" and \"drama,\" which suggest that the sentiment is positive. Additionally, the phrase \"history lesson\" implies that the film is educational and informative, which is generally seen as a positive thing. Therefore, the sentiment of the input is positive.SENTIMENT: PositiveINPUT: press the delete keyCLUES: -Positive: None. -Negative: \"delete key.\"REASONING: The input contains the phrase \"delete key,\" which implies that something is being removed or erased. This is generally seen as a negative action, suggesting that the sentiment of the input is negative.SENTIMENT: Negative\\n(c)Figure 2: Examples of few-shot ( k=1) prompting methods for the text classiﬁcation task: (a)represents for the\\nvanilla prompting method; (b)denotes for the Chain-of-Thought (CoT) (Kojima et al., 2022) prompting method;\\n(c)represents for the proposed CARP prompting method.\\nﬁrst category is the encoder-only model like\\nBERT (Devlin et al., 2018). BERT (300M) (Devlin\\net al., 2018) and its variants (Liu et al., 2019;\\nSun et al., 2020; Clark et al., 2020; Feng et al.,\\n2020; Sun et al., 2021) adopt the pre-training then\\nﬁne-tuning paradigm for NLP tasks: use masked\\nlanguage models as the main training objective for\\npretraining, and ﬁne-tune the pretrained model in\\nthe annotated downstream datasets.\\nThe second category is the decoder-only models\\nlike GPT (Radford et al., 2019a). GPT (Radford\\net al., 2019a) uses the decoder of an auto-\\nregressive transformer (Vaswani et al., 2017)\\nmodel for predicting the next token in a sequence.\\nGPT (Radford et al., 2019a) and its variants (Daiet al., 2019; Keskar et al., 2019; Radford et al.,\\n2019b; Chowdhery et al., 2022; Zhang et al.,\\n2022a) also follow the pre-training then ﬁne-tuning\\nparadigm. GPT-3 (175B) (Brown et al., 2020)\\nproposes to formalize all NLP tasks as generating\\ntextual responses condition on the given prompt.\\nThe third category is the encoder-decoder\\nmodels like T5 (Raffel et al., 2020). T5\\n(11B) (Raffel et al., 2020) and its variants (Lewis\\net al., 2019; Xue et al., 2020) are encoder-decoder\\ntransformer models, which generate new sentences\\ndepending on a given input, following the pre-\\ntraining then ﬁne-tuning paradigm.', metadata={'source': './text_classification_with_llm.pdf', 'page': 2}),\n Document(page_content='2.2 In-context Learning\\nUnlike the pre-training then ﬁne-tuning\\nparadigm (Devlin et al., 2018), which saves\\nmodel weights and uses task-speciﬁc datasets\\n(i.e., train/valid/test set), in-context learning (ICL)\\ngenerates textual responses (i.e., label words)\\nconditioning on the given prompt (usually) with a\\nfew annotated examples for downstream tasks.\\nLi and Liang (2021); Zhong et al. (2021); Qin\\nand Eisner (2021) propose to optimize prompts\\nin the continuous space. Rubin et al. (2021);\\nDas et al. (2021); Liu et al. (2021); Su et al.\\n(2022) introduce different strategies for selecting\\nin-context examples. Lampinen et al. (2022) show\\nthat explanations of examples in a few-shot prompt\\nlead to a performance boost. Marasovi ´c et al.\\n(2021) ﬁnd that GPT-3 outperforms other models\\nby a large margin in the explanation generation\\ntask. Wei et al. (2022b) propose chain-of-thought\\nreasoning and utilized <input, chain-of-thought,\\noutput> triples as the prompt for LLMs. Wiegreffe\\net al. (2021) traine a supervised ﬁlter to select\\nexplanations generated by GPT-3 on the SNLI and\\nCommonsenseQA tasks.\\n2.3 Text Classiﬁcation\\nText classiﬁcation is a task that aims to assign\\npredeﬁned labels (e.g., sentiment polarity, topic,\\netc) to a given text. Earlier work decouple the\\ntask into two steps: (1) extract features using\\nneural models such as RNNs (Irsoy and Cardie,\\n2014; Yang et al., 2016; Wang et al., 2018; Liu\\net al., 2016; Xie et al., 2020), CNNs (Kim, 2014;\\nZhang et al., 2015; Lai et al., 2015; Conneau\\net al., 2016; Wei and Zou, 2019), GCN (Yao et al.,\\n2019), LLMs (Howard and Ruder, 2018; Sun et al.,\\n2019; Chai et al., 2020; Chen et al., 2020; Lin\\net al., 2021); and (2) feed extracted features into\\na classiﬁer (Joulin et al., 2016) to obtain the ﬁnal\\nlabel.\\nRecently, in-context learning has achieved\\nsuccess and changes the paradigm in the text\\nclassiﬁcation task. Schick and Schütze (2020)\\nreformulate input examples into cloze-style phrases\\nand annotate the unlabeled text. Han et al. (2021)\\ndesign sub-prompts and applied logic rules to\\ncompose sub-prompts into ﬁnal prompts. Liu\\net al. (2021) retrieve semantically-similar examples\\nto a test sample to formulate its corresponding\\nprompt. Shi et al. (2022) retrieve label-words-\\nsimilar examples as demonstrations in prompts.3 Prompt Construction\\n3.1 Overview\\nWe follow the standard prompt-based in-context\\nlearning paradigm. Given an input sequence\\nxinput =fx1;x2;:::;x lg, the task of assigning a\\ntext-class label to an input text is transformed to\\ngenerating a pre-deﬁned textual response y2Y verb\\n(e.g., positive, negative, etc) conditioning on the\\nprompt xprompt using a language model.\\n3.2 Prompt Construction\\nThe prompt xprompt , which is constructed based on\\nx, consists of the following three components:\\n(1) Task description xdesc generally describes\\nthe task. For different classiﬁcation tasks, e..g,\\nsentiment classiﬁcation, topic classiﬁcation, etc,\\ndescriptions are different. Take the sentiment\\nclassiﬁcation task as an example, the task\\ndescription is given as follows:\\nClassify the overall sentiment of the input as\\npositive or negative\\n(2) Demonstration consists of a sequence of\\nannotated examples:\\nf(x1\\ndemo;y1\\ndemo);:::; (xk\\ndemo;yk\\ndemo)g\\nwhere xj\\ndemo;1\\x14j\\x14kdenotes the jth\\ninput sequence and yj\\ndemodenotes the text which\\nis transformed from the label, e.g., positive or\\nnegative for the binary sentiment classiﬁcation\\ntask. Demonstration serves as two purposes: (1)\\nproviding the LLM with evidence to consult on\\nfor decision making, which will signiﬁcantly boost\\nperformances; (2) provides an output format that\\nLLM’s outputs need to follow, so that the output,\\nwhich takes the form of natural language, can be\\nfurther easily transformed to labels. It is worth\\nnoting that demonstrations are only needed for the\\nfew-shot learning setup, but not for the zero-shot\\nlearning setup.\\n(3) Input xinput is the test text sequence to\\nclassify.\\nThe prompt xprompt for a test input is\\nconstructed by concatenating the task\\ndescription xdesc, a sequence of demonstrations\\nf(x1\\ndemo;y1\\ndemo);:::; (xk\\ndemo;yk\\ndemo)g, and the test\\nsequence xtest, which can be given as follows:\\nfxdesc;\\\\n;<demo>1;\\\\n;:::;<demo>k;\\\\n;xtestg', metadata={'source': './text_classification_with_llm.pdf', 'page': 3}),\n Document(page_content='3.3 Demonstration Sampling\\nThe few-shot setup requires demonstrations\\nsampled from the training set. Strategies that we\\nexplore include:\\nRandom Sampling a straightforward strategy\\nfrom samplings is to randomly sample kexamples\\nf(x1;y1);:::; (xk;yk)gfrom the training set Dtrain\\nfor a text sequence xtest.\\nkNN Sampling The key disadvantage for\\nrandom sampling is that there is no guarantee that\\nselected samples are semantically related to the\\ninput sequence. One straightforward alternative\\nis to sample examples that are similar to the test\\nsequence using kNN search (Khandelwal et al.,\\n2020). In this process, the test sequence xtestis\\nﬁrst mapped to a vector vtestusing an encoder\\nmodelf. Then using vtestas the query, we search\\nthrough the entire training set Dtrainto retrievek\\nnearest text sequence to get knearest data examples\\nN=fxj;yjgk\\nj=1as demonstrations. We use\\nthe following encoder models to obtain sentence\\nrepresentations and similarity scores:\\nSimCSE (Gao et al., 2021) is a contrastive\\nlearning model for sentence embeddings. We use\\nSup-SimCSE-RoBERTa-Large model as an\\nencoder model, which is initizlied with RoBERTa-\\nLarge (Liu et al., 2019) and ﬁne-tuned on the\\nnatural language inference datasets. SimCSE (Gao\\net al., 2021) is a semantic-based model and\\nretrieves semantically similar examples, but not\\nnecessarily examples with the same labels.\\nFinetuned Model FT for short. The key\\ndisadvantage for SimCSE (Gao et al., 2021) and\\nother general semantic encoding models (Reimers\\nand Gurevych, 2019; Seonwoo et al., 2022; Sun\\net al., 2022) is that it measures the general semantic\\nsimilarity but is not speciﬁcally tailored to the text\\nclassiﬁcation task. To resolve this issue, CARP\\nuses the model ﬁne-tuned on the training dataset as\\nthekNN encoder model. Speciﬁcally, we ﬁrst ﬁne-\\ntune a Roberta model on the training data. Next\\nwe use the [CLS] embedding as the sentence level\\nrepresentation for KNN search. Since the ﬁne-\\ntuned model is trained based on task-speciﬁc labels,\\nit guarantees that retrieved samples are close to the\\ninput sequence with respect to the task. Using ﬁne-\\ntuned model provides a channel to connect LLMs\\nwith the full training set, in spite of the limited\\nnumber of tokens allowed in demonstrations. Thisstrategy lets the model take the advantage of both\\nthe LLMs’ generalization abilities and all task-\\nspeciﬁc evidence provided by the training dataset.\\n4 Clues Collecting and Reasoning\\nTo enhance the models’ reasoning ability in\\naddressing linguistic phenomenon tailored to text\\nclassiﬁcation, we propose a progressive reasoning\\nstrategy that involves clue collection, reasoning and\\ndecision making. This process also mimics how\\nhuman decisions: where we ﬁrst collect evidence\\nfrom the input, separating chaff from wheat; next\\nwe piece together local evidence to form a global\\npicture, which leads to ﬁnal decision making.\\nNext we ﬁrst given an overview of the the clue\\ncollecting and reasoning process, and then describe\\nimplementation details.\\n4.1 Overview\\nCollecting Clues For a test sequence, clues\\nare local fact evidence such as keywords,\\nphrases, contextual information, semantic meaning,\\nsemantic relationships, tones, references, etc. The\\nfollowing is an example for clues of an input:\\nInput :Steers turns in a snappy screenplay that\\ncurls at the edges; it’s so clever you want to hate\\nit.\\nClues :\"snappy\", \"clever\", \"want to hate it\" are\\nclues for determining the sentiment of the input\\nsentence.\\nReasoning For reasoning, the LLM is prompted\\nto go beyond superﬁcial keywords to mine deeper\\nperspectives, considering language phenomenon\\nsuch as negation, intensiﬁcation, irony, etc), and\\npiece together local evidence to form the ﬁnal\\ndecision. The following example shows the\\nreasoning process to decide the sentiment of the\\nabove example based on the evidence collected:\\n1. The phrase \"snappy screenplay\" implies that the\\nscreenplay is of a high quality and is well-crafted.\\n2. The phrase \"curls at the edges\" implies that the\\nscreenplay is cleverly written.\\n3. The phrase \"so clever you want to hate it\" is\\na paradoxical statement, which suggests that the\\nsentiment is positive despite the use of the word\\n\"hate\".\\nDecision Making Based on the reasoning\\nprocess, the model makes the decision for the\\nsentiment of the given input:', metadata={'source': './text_classification_with_llm.pdf', 'page': 4}),\n Document(page_content='Overall, the clues and reasoning process point\\nto a positive sentiment for the input sentence.\\nThe merits for the incorporation of clue ﬁnding\\nand reasonings are as follows: (1) it prompts the\\nmodel to progressively think and make decisions:\\nclue ﬁnding focuses more on superﬁcial features\\nsuch as keywords, while reasoning makes deeper\\njustiﬁcations based on superﬁcial features. This\\nprocess better mimics how we humans decide;\\n(2) clue ﬁnding and reasoning serve as a tunnel\\nto let human intervene: in the few-shot setup,\\nwhere clues and reasons need to be prepared\\nin advance for demonstrations, we can modify\\nthem as we see ﬁt. This is extremely helpful for\\ntrouble shooting in the prompt-construction stage\\nfor error corrections; (3) from an interpretation\\nand uncertainty estimation perspective, clues\\nand reasoning in few-shot setups are human-\\nreadable inﬂuence functions; (4) in contrast to\\nlist annotated (text, label) pairs in few-shot\\nsetups, incorporating clues and reasoning process\\nin prompts aligns closer with the instruction tuning\\nobjective. The discrepancy between LLMs training\\nobjectives and in-context learning for downstream\\ntasks has been reduced.\\n4.2 Collecting clues and reasoning in\\nzero-shot\\nIn the zero-shot setup, as no demonstration is\\nallowed, no concrete example for clues and reasons\\ncan be provided. In this way, we only add requests\\nasking the model to output clues and reasons in the\\nprompt. The prompt is given as follows:\\nThis is an overall sentiment classiﬁer for\\nopinion snippets.\\nFirst, list CLUES (i.e., keywords, phrases,\\ncontextual information, semantic relations,\\nsemantic meaning, tones, references) for\\ndetermining the overall sentiment of the input.\\nNext, deduce a diagnostic reasoning process\\nfrom clues and the input to determine the\\noverall sentiment.\\nFinally, determine the sentiment of input as\\nPositive or Negative considering clues, the\\nreasoning process and the input.\\nINPUT: <text>\\nCLUES:4.2.1 Clue Collecting and Reasoning in\\nfew-shot\\nIn the few-shot setup , we need to prepare clues\\nand reasonings for all examples in the training set\\nin advance as all training examples have chances\\nto be selected as demonstrations given different\\ntest inputs. Previous efforts in math problems (Wei\\net al., 2022b; Kojima et al., 2022; Ye and Durrett,\\n2022; Zhang et al., 2022b) prepare hand-drafted\\nreasoning for a few examples, and always use these\\nexample as demonstrations. This strategy does\\nnot ﬁt for our situation as it is extremely time-\\nintensive to manually generate clues and reasonings\\nfor all training examples, To resolve this issue, we\\nharness LLMs for automatic clue and reasoning\\ngeneration, where we ask LLMs to generate clues\\nand reasoning based on both the input and its\\ncorresponding label.\\nClue Generation For a given training example\\n<text> paired with the label word <label-word>\\n(e.g., positive), we ask LLM to generate clues that\\nindicate the label:\\nList CLUES (i.e., keywords, phrases, contextual\\ninformation, semantic meaning, semantic\\nrelationships, tones, references) that support\\nthe sentiment determination of the input (limit\\nto 15 words).\\nINPUT: <text>\\nSENTIMENT: <label-word>\\nReasoning Generation Based on clues\\ngenerated clues, the input, and the label, we ask\\nLLMs to generate reasoning details2:\\nBased on the input and clues, articulate the\\ndiagnostic reasoning process that supports the\\nsentiment determination of the input.\\nINPUT: <text>\\nLABEL: <label-word>\\nCLUES: <clues>\\nREASONING:\\nGiven the generated clues and reasonings\\nfor all training examples, at test time, when\\nK-nearest examples are selected demonstrations,\\nits corresponding clues and reasons are\\nconcatenated to the demonstration. In this\\nway, each demonstration example is composed\\n2LLMs often generate long responses, in order to ensemble\\nmore demonstrations in prompts, we use \"limit to 50 words\" .\\nAfter conducting an analysis of the generated responses, we\\nﬁnd that LLMs can explain the reason within limited words.', metadata={'source': './text_classification_with_llm.pdf', 'page': 5}),\n Document(page_content='INPUT & GOLD LABEL CLUES REASONING\\nINPUT : johnnie to and wai ka\\nfai are sure to ﬁnd an enthusiastic\\naudience among american action\\nadventure buffs, but the ﬁlm ’s\\ninterests may be too narrow to\\nattract crossover viewers\\nGOLD LABEL : Negative- Positive Clues: enthusiastic, action,\\nadventure.\\n- Negative Clues: narrow, crossoverThe diagnostic reasoning process supporting the\\nsentiment determination of the input is that the phrase\\n\"johnnie to and wai ka fai are sure to ﬁnd an enthusiastic\\naudience among american action adventure buffs\"\\nsuggests that the ﬁlm is likely to be well-received by\\nthis speciﬁc demographic. However, the subsequent\\nphrase \"but the ﬁlm’s interests may be too narrow to\\nattract crossover viewers\" implies that the ﬁlm is unlikely\\nto appeal to a broader audience, suggesting a negative\\nsentiment. Therefore, the overall sentiment of the input\\nis negative.\\nINPUT : moore sell globe unit\\nmoore resources inc said agreed\\nprinciple sell globe inc unit cyprus\\nminerals co undisclosed terms\\nsale expected close mid april\\nsubject certain conditions including\\nboard approval companies globe\\nproduces silicon metals products\\nused chemical aluminum iron rubber\\nsemiconductor industries reuter\\nGOLD LABEL : Acquisitions- Acquisition (agreed, sell, unit, sale,\\nclose, board approval)\\n- Companies (Moore Resources Inc,\\nGlobe Inc, Cyprus Minerals Co)\\n- Products (silicon, metals, chemical,\\naluminum, iron, rubber, semiconductor)\\n- Reuter (news agency)The input mentions an acquisition agreement between\\nMoore Resources Inc and Globe Inc, and the sale\\nis expected to close in mid-April, suggesting an\\nAcquisitions topic. The input also mentions Cyprus\\nMinerals Co, silicon and metals products which are used\\nin chemical, aluminum, iron, rubber, and semiconductor\\nindustries, and a Reuter news agency, all of which\\nsupport the Acquisitions topic.\\nTable 1: Examples of generated clues and reasoning for demonstrations.\\nby a (text, clues, reasons, golden\\nlabel word) pair. The prompt is thus given as\\nfollows:\\nThis is a sentiment classiﬁer for input opinion\\nsnippets.\\nList CLUES (i.e., keywords, phrases, contextual\\ninformation, semantic meaning, semantic\\nrelationships, tones, references) that support\\nthe sentiment determination of the input.\\nNext, deduce the diagnostic REASONING\\nprocess from premises (i.e., clues, input) that\\nsupport the sentiment determination.\\nFinally, based on clues, the reasoning and the\\ninput, categorize the overall SENTIMENT of\\ninput as Positive or Negative.\\ninput: <demo-text-1>\\nclues: <demo-clues-1>\\nreasoning: <demo-reason-1>\\nsentiment: <demo-label-word-1>\\ninput: <demo-text-2>\\nclues: <demo-clues-2>\\nreasoning: <demo-reason-2>\\nsentiment: <demo-label-word-2>\\n... ...\\ninput: <demo-text-n>\\nclues: <demo-clues-n>\\nreasoning: <demo-reason-n>\\nsentiment: <demo-label-word-n>\\ninput: <text>\\nExamples for prompts with clues and reasons are\\nshown in Figure 2. In this way, for a test example,\\nby following the format of demonstrations, theLLM will ﬁrst output clues, then reasons, and at\\nlast decisions.\\n4.3 Voting\\nUnlike conventional discriminative models for text\\nclassiﬁcation, which generate deterministic results\\nduring inferences, LLMs for in-context learning\\nare generative models and generate distinct textual\\nresponses with diverse sampling strategies in\\nmultiple runs. We consider the following voting\\nstrategies in the paper:\\n•Majority Vote : the ﬁnal result is the most\\nfrequent prediction among multiple runs.\\n•Weighted Probability Vote : the ﬁnal result\\nis the one with weighted summed probability\\nfrom multiple runs.\\n5 Experiments\\nIn order to evaluate the effectiveness of the\\nproposed method, we conduct experiments on two\\nsetups: (1) full training setup, where the model has\\nthe access to the full training data; and (2) low-\\nresource setup, where the model can only access\\npartial training dataset. The low-resource setup\\nbetter mimics real-world situations where training\\ndata is limited. For the full training setup, we\\nfollow the standard train/dev/test split. For the low-\\nresource setup, we randomly sample ninstances\\nper class (ninf16;128;256;512;1024g) from the\\nbenchmark training set. The sampled subset forms\\na new training set to test different models’ abilities\\nin the low-resource situations. During experiments,\\nwe train models/sample demonstrations with the', metadata={'source': './text_classification_with_llm.pdf', 'page': 6}),\n Document(page_content='SST-2 AGNews R8 R52 MR Average\\nSupervised Methods\\nRoBERTa-Large (Liu et al., 2019) 95.99 95.55 97.76 96.42 91.16 95.38\\nDeBERTa (He et al., 2020) 94.75 95.32 98.33 96.32 90.19 94.99\\nRoBERTa-GCN (Lin et al., 2021) 95.80 95.68* 98.2 96.1 89.7 95.10\\nXLNet (Yang et al., 2019) 96.10* 95.55 - - - -\\nVLAWE (Ionescu and Butnaru, 2019) - - - - 93.3* -\\nGCN-SB (Zeng et al., 2022) - - 98.53* 96.35* 87.59 -\\nZero-shot Setting\\nVanilla (Brown et al., 2020) 91.55 90.72 90.19 89.06 88.69 90.04\\nCoT (Kojima et al., 2022) 92.11 91.25 90.48 91.24 89.37 90.89\\nCARP 93.01 92.60 91.75 91.80 89.94 91.82\\nFew-shot Setting ( k=16)\\nRandom Sampler\\nVanilla (Brown et al., 2020) 92.36 91.74 91.58 91.56 89.15 91.28\\nCoT (Kojima et al., 2022) 94.56 95.02 92.49 92.03 89.91 92.80\\nCARP 96.20 95.18 97.60 96.19 90.03 95.04\\nSimCSEkNN-Sampler\\nVanilla (Brown et al., 2020) 93.90 93.50 94.36 92.40 89.59 94.05\\nCoT (Kojima et al., 2022) 94.21 94.28 95.07 92.98 90.27 93.69\\nCARP 95.69 95.25 97.83 96.27 90.74 95.16\\nFTkNN-Sampler\\nVanilla (Brown et al., 2020) 94.01 94.14 95.57 95.79 90.90 94.08\\nCoT (Kojima et al., 2022) 95.48 94.89 95.59 95.89 90.17 94.40\\nCARP 96.80 95.99 98.29 96.82 91.90 95.97\\nCARP (WP V ote) 97.39 96.40 98.78 96.95 92.39 96.38\\nTable 2: Accuracy performances of different settings on benchmarks. We report mean and standard deviation\\nresults over 5 runs. The GPT-3 denotes text-davinci-003 . In few-shot experiments, we sample 16 annotated\\nexamples (k=16) for every test instance. *indicates previous state-of-the-art results. \"MJ V ote\" is short for majority\\nvote. \"WP V ote\" denotes weighted probability vote.\\nnew training set.\\nWe conduct experiments on ﬁve widely-used\\ndatasets, including SST-2 (Socher et al., 2013), R8,\\nR523, AGNews (Zhang et al., 2015) and Movie\\nReview (MR) (Pang and Lee, 2005). More details\\nof the benchmarks and low-resource datasets can\\nbe found in Appendix ??.\\nFor zero-shot and few-shot experiments,\\nwe use InstructGPT-3 (Ouyang et al., 2022)\\n(text-davinci-003 , 175B) as the backbone.\\nDue to the input token limitation, we use\\nk= 16 for few-shot setups. Prompts on the\\nﬁve datasets are shown in Appendix ??. Model\\nhyper-parameters can be found in Table 34.\\nWe use Vanilla to denote the conventional ICL\\napproach where LLMs are directly prompted to\\ngenerate labels. We use CoT (Kojima et al., 2022)\\n3R8 and R52 are original from https://www.cs.umb.\\nedu/~smimarog/textmining/datasets/\\n4During experiments, we ﬁnd that CARP is robust with\\ndifferent hyper-parameters. Experimental results can be found\\nin Appendix B.2to denote the baseline that mimics the chain-of-\\nthought strategy and use CARP to denote the\\nproposed method.\\n5.1 Models for Comparison\\nSupervised models trained on the trained set\\nnaturally constitute baselines to compare with.\\nWe use the following models as baselines, and\\nmore details of hyper-parameters are shown in\\nAppendix B.1:\\n•RoBERTa-Large :We ﬁne-tune RoBERTa-\\nLarge (Liu et al., 2019) on the training set.\\n•RoBERTa-GCN :Lin et al. (2021) constructs\\nheterogeneous graph networks on top of the\\nRoBERTa-Large (Liu et al., 2019) model.\\n•DeBERTa :He et al. (2020) improve\\nRoBERTa by using disentangled attention\\nmechanism and an enhanced mask decoder.\\n•XLNet :Yang et al. (2019) propose a\\ngeneralized autoregressive pretraining\\nmethod that enables learning bidirectional', metadata={'source': './text_classification_with_llm.pdf', 'page': 7}),\n Document(page_content='Parameter Value\\nEngine Name text-davinci-003\\nMax Tokens 200\\nTemperature 0.7\\nTop P 1\\nFrequency Penalty 0.0\\nPresence Penalty 0.0\\nBest Of 1\\nTable 3: OpenAI API Hyper-parameters.\\ncontexts.\\n•GCN-SB :Zeng et al. (2022) propose a\\nsimpliﬁed boosting algorithm, which makes\\nCNN learn the samples misclassiﬁed by GCN\\nagain.\\n•VLA WE :Ionescu and Butnaru (2019) obtain\\ndocument embeddings based on aggregating\\nthe differences between each codeword vector\\nand each word vector (from the document)\\nassociated to the respective codeword.\\nFew-shot Setup For demonstration sample\\nstrategies in the few-shot setup, we consider the\\nfollowing strategies for comparison: (more details\\ncan be found in Section 3.3):\\n•Random Sampler : randomly samples k\\nexamples.\\n•SimCSEkNN-Sampler : samplesknearest\\nexamples based on SimCSE (Gao et al., 2021)\\nrepresentations5.\\n•FTkNN-Sampler : sample knearest\\nexamples using Fine-Tuned RoBERTa-Large\\nrepresentations.\\n5.2 Results on the full training set\\nExperimental results are shown in Table 2. As\\ncan be seen, performances of few-shot setups\\nconsistently outperform zero-shot setups. In terms\\nof sampling strategies in the few-shot setups, we\\nobserve that simcse KNN-sampler outperform\\nrandom sampler, illustrating the importance of\\nadding demonstrations that are relevant to the test\\ninput in the few-shot setup. We also observe\\nthat FT KNN-sampler consistently outperforms\\nsimcse KNN-sampler. This shows that, the ﬁne-\\ntuned model, which takes the advantage of the full\\ntraining set, serves as a better retriever for task-\\nspeciﬁc demonstration retrieval than the general-\\n5Speciﬁcally, we use Sup-SimCSE-RoBERTa-Large\\nas the text encoder.purposed simcse retriever.\\nFor different reasoning strategies, we ﬁrst\\nobserve that the CoT strategy outperforms the\\nvanilla strategy, which straightforwardly asks\\nLLMs to generate results without further reasoning\\nsteps. CARP consistently outperforms CoT across\\nall benchmarks, i.e., +1.48, +0.97, +2.76, +\\n3.29, +0.47 respectively on SST-2, AGNews,\\nR8, R52 and MR datasets. This demonstrates\\nthe necessity of building models with complex\\nlinguistic phenomena involved in text classiﬁcation,\\nand the effectiveness of CARP in doing this job.\\nCompared with supervised learning baselines,\\nwe ﬁnd that the vanilla model using LLM\\nunderperforms supervised baselines, while few-\\nshot CoT is able to obtain slightly worse or\\ncomparable results agains supervised baselines.\\nNotably, single CARP outperforms ﬁne-tuned\\nRoBBERTa on all benchmarks. Using WP voting\\nstrategies, CARP yields new SOTA performances\\non four out of the 5 datasets, 97.39 on SST-2\\n(+1.24), 96.40 (+0.72) on AGNews, 98.78 (+0.25)\\non R8 and 96.95 (+0.6) on R52, and a performance\\ncomparable to SOTA on MR (92.39 v.s. 93.3).\\n5.3 Results on low-resource settings\\nTo estimate low-resource circumstances, we\\nsamplen=f16;128;256;512;1024ginstances\\nfor each class as low-resource setups. Experimental\\nresults are shown in Table 4. As can be seen,\\nwhen the training set size is extremely small (i.e.,\\n16 or 128 sentences), and the performance of the\\nsupervised model is far below CARP. Even with\\nonly 16 examples to train on, the accuracy of CARP\\nof SST-2 already around 90%, whereas supervised\\nmodels’ performance is similar to random guess.\\nThis demonstrates the strong generalization ability\\nof CARP in the low-resource setup. As we\\nanticipated, the kNN search efﬁciency improved\\nat a faster rate as the amount of the training\\ndata increases; Enlarging the training dataset\\nincreases the chances that the chosen examples\\nwill correspond to the input, resulting in improved\\nresults. Speciﬁcally, using 16 examples per\\nclass, CARP achieves comparable performances to\\nsupervised models with 1,024 examples per class;\\nusing 512 instance per class annotation data, CARP\\nachieves comparable performances to supervised\\nmodels trained on the full set.', metadata={'source': './text_classification_with_llm.pdf', 'page': 8}),\n Document(page_content='Dataset Model n=16n=128n=256n=512n=1024\\nFT RoBERTa 51.52 52.31 53.89 70.49 90.30\\nSST-2GPT-3 Vanilla 90.15 90.36 91.70 93.86 94.68\\nGPT-3 Zero-shot-CoT 89.66 90.19 90.80 94.42 94.89\\nGPT-3 CRAP 90.48 91.07 91.77 94.03 95.20\\nAGNewsFT RoBERTa 21.87 38.19 40.08 50.18 78.09\\nGPT-3 Vanilla 89.47 89.63 90.54 93.02 94.79\\nGPT-3 Zero-shot-CoT 89.66 90.16 91.70 94.86 95.28\\nGPT-3 CRAP 90.16 90.94 91.07 94.08 95.48\\nR8FT RoBERTa 11.29 48.19 60.18 70.70 88.68\\nGPT-3 Vanilla 89.15 90.27 91.70 94.00 94.91\\nGPT-3 Zero-shot-CoT 90.49 90.88 91.81 95.42 95.75\\nGPT-3 CRAP 90.23 91.03 91.77 95.56 96.67\\nR52FT RoBERTa 38.29 39.10 59.18 67.19 81.53\\nGPT-3 Vanilla 89.15 90.04 90.29 91.88 92.06\\nGPT-3 Zero-shot-CoT 89.46 90.02 90.73 93.20 94.12\\nGPT-3 CRAP 90.82 91.00 95.85 94.36 96.27\\nMRFT RoBERTa 51.20 52.11 53.58 68.29 88.37\\nGPT-3 Vanilla 86.04 88.68 88.99 89.80 90.18\\nGPT-3 Zero-shot-CoT 86.26 89.00 90.01 90.16 90.89\\nGPT-3 CRAP 86.54 87.19 89.63 90.01 91.20\\nTable 4: Experimental results on low-resource ( nexample per class) settings. We compare ﬁne-tuned RoBERTa-\\nLarge with 16-shots GPT-3 setting. For GPT-3, we use SimCSE (Gao et al., 2021) to retrieve 16 annotated examples\\nfrom the low-resource train set. \"cls\" represents GPT-3 makes decisions by generating label words; \"reason-cls\"\\ndenotes that GPT-3 ﬁrst generates the reasoning process and then makes decisions; \"clue-reason-cls\" represents\\nthat GPT-3 ﬁnds clues in the given text, then explain the reasoning process and ﬁnally makes decisions.\\n5.4 Domain Adaptation\\nIt is unclear whether it is essential to train\\nmodels on the speciﬁc dataset for retrieving\\ndemonstrations. In this subsection, we conduct\\nan analysis on using demonstrations from out-of-\\ndistribution datasets.\\nWe use SST-2 and Yelp, and the task is\\nto determine the positive or negative polarity\\nof the given text. SST-2 and Yelp are from\\ndifferent domains: SST-2 are snippets from Rotten\\nTomatoes6, whereas Yelp7consists of product\\nreviews from the online website. Experimental\\nresults are shown in Table 5. SST-2 train & SST-2\\ntest means that demonstrations are from the SST-2\\ndataset and test is performed on SST-2 dataset; Yelp\\ntrain & SST-2 test means demonstrations are from\\nyelp and test is performed on SST-2 dataset. We see\\na signiﬁcant decrease (-7.2%, 95.99% v.s.88.78%\\n) in performance when switching SST-2 train to\\nYelp-2 train using supervised RoBERTa, which\\nillustrates that supervised models are very sensitive\\n6https://www.rottentomatoes.com/\\n7https://drive.google.\\ncom/drive/folders/0Bz8a_\\nDbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M?\\nresourcekey=0-TLwzfR2O-D2aPitmn5o9VQ&\\nusp=share_linkFT RoBERTa on FT RoBERTa on\\nSST-2 Train Yelp Train\\nSST-2 Test 95.99 88.78\\nYelp Test 92.38 96.04\\nCARP with CARP with\\nSST-2 demon. Yelp demon.\\nSST-2 Test 96.80 96.29\\nYelp Test 95.94 96.32\\nTable 5: Results for Yelp test set when using\\nin-domain/out-of-domain kNN sampler and\\ndemonstrations source. We use FT kNN Sampler\\nto retrieve demonstrations on the corresponding train\\nset.\\nto the out-of-distribution data. On the contrary,\\nwe only observe a slight decrease in performance\\n(-0.5%, 96.80% v.s. 96.29%) when switching SST-\\n2 train to Yelp-2 train on SST-2 test, illustration\\nthe greater capabilities of CARP on the domain\\nadaptation situations.\\nThis means CARP is very robust when training\\nand test are not from the same domain. On the\\ncontrary,', metadata={'source': './text_classification_with_llm.pdf', 'page': 9}),\n Document(page_content='SST-2 AGNews R8 R52 MR Average\\nSupervised Methods\\nRoBERTa-Large 95.99 95.55 97.76 96.42 91.16 95.38\\nRoBERTa-GCN 95.80 95.68 98.2 96.1 89.7 95.10\\nZero-shot Setting\\nVanilla 91.55 90.72 90.19 89.06 88.69 90.04\\nZero-shot-CoT 92.11 91.25 90.48 91.24 89.37 90.89\\nCARP 94.41 93.18 93.29 92.69 90.03 92.72\\nFew-shot Setting\\nRandom Sampler\\nVanilla 91.36 91.48 90.60 90.68 89.15 90.65\\nZero-shot-CoT 92.56 92.65 92.49 92.03 89.91 91.93\\nCARP 94.41 93.18 93.29 92.69 90.03 92.72\\nSimCSEkNN-Sampler\\nVanilla 93.90 93.50 94.36 92.40 89.59 92.75\\nZero-shot-CoT 94.21 94.28 95.07 92.98 90.27 93.36\\nCARP 95.99 95.53 95.31 93.84 90.64 94.26\\nFTkNN-Sampler\\nVanilla 94.01 94.14 95.57 95.79 90.90 94.08\\nZero-shot-CoT 95.48 94.89 95.59 95.89 90.17 94.40\\nCARP 96.62 95.97 98.13 96.12 91.86 95.74\\nTable 6: Accuracy performances of different settings on test subsets (results are over 5 runs). GPT-3 denotes\\ntext-davinci-003 . In few-shot experiments, we sample 16 annotated examples ( k=16) per prompt. \"MJ\\nV ote\" is short for majority vote. \"WP V ote\" denotes weighted probability vote.\\n6 Ablation Studies\\nIn this section, we conduct comprehensive ablation\\nstudies to get a better knowledge about different\\nelements of CARP.\\n024 8 12 16 20 2492939495969798\\nNumber of DemonstrationsTest Accuracy (%)\\nRandom Sampler\\nSimCSEkNN-Sampler\\nFTkNN-Sampler\\nFigure 3: Performances v.s. the number of\\ndemonstrations in few-shot prompts.\\n6.1 Impact of the number of demonstrations\\nWe explore the effect of the number of\\ndemonstrations in prompts. We conduct\\nexperiments on the SST-2 dataset. Results for\\nthe vanilla prompting and the CARP schemas\\nusing different sampling strategies are shown in\\nFigure 3 and Figure 4, respectively. As can be024 8 12 16 20 2492939495969798\\nNumber of DemonstrationsTest Accuracy (%)\\nRandom Sampler\\nSimCSEkNN-Sampler\\nFTkNN-Sampler\\nFigure 4: Performances v.s. the number of\\ndemonstrations in few-shot prompts for the CARP\\nstrategy, where LLMs are ﬁrst asked to generate\\nevidence, then to reason and at last to generate ﬁnal\\nresults.\\nseen, performances improve as the number of\\ndemonstrations increases for both the vanilla and\\nthe CARP schemas.\\n6.2 The effect of components in\\ndemonstrations\\nCARP uses (text, clues, reasons,\\ngolden label word) pairs as\\ndemonstrations. In this subsection, we exploit the\\ninﬂuence of each component in (text, clues,', metadata={'source': './text_classification_with_llm.pdf', 'page': 10}),\n Document(page_content='Prompts SST-2 R8\\nCARP 96.80 98.29\\nw/o Text 92.28 94.18\\nw/o Clue 95.48 95.29\\nw/o Reason 95.72 97.82\\nw/o Label 96.53 98.18\\nTable 7: The effect of components on the SST-2 dataset\\nwith different strategies.\\nreasons, golden label word) by\\nremoving it from prompts. Experimental results\\nare shown in Table 7. As shown in Table 7,\\ntext in demonstrations has the biggest inﬂuence\\nimpact of the ﬁnal results. When (text, clue,\\nreason) as demonstrations, the label has\\neffect to the performances.\\n6.3 The effect of different types of label\\nwords\\nLabel words denote words generated by LLMs that\\nindicate the label of the input. In this subsection,\\nwe explore the impact of using different kinds of\\nlabel words:\\n•Position index : number of index. i.e., one,\\ntwo, three and etc to denote the label.\\n•Annotation words : words used to refer to the\\ncategory in the annotation ﬁle. e.g., positive,\\nnegative.8\\n•Synonyms words : synonyms words e.g.,\\ngreat, terrible.\\n•Flipped words : words that are contrary to\\noriginal target meanings. e.g., \"positive\" to\\ndenote the negative polarity, \"negative\" to\\ndenote the positive polarity.\\n•Random words : randomly choose words in\\nthe vocabulary. e.g., order, number.\\n•Special tokens : tokens that do not have\\nsemantic meaning. They are independent of\\nthe input and added for a certain purpose. e.g.,\\n<cls>, <mask>.\\nResults are shown in Table 8. As can be seen,\\nfew-shot ICL with annotation words as label words\\nachieves the best performances. It is also worth\\nnoting that we observe a signiﬁcant performance\\ndecrease when ﬂipped words are used as label\\nwords in demonstrations.\\n8GPT-3 generates the same label words for binary\\nsentiment classiﬁcation task.Strategy Label Words(+,-) CARP\\nPosition Index One, Two 95.66\\nAnnotation Words Positive, Negative 96.86\\nSynonyms Words Great, Terrible 96.27\\nFlipped Words Negative, Positive 64.63\\nRandom Words Cf, Ng 95.06\\nSpecial Tokens <POS>, <NEG> 96.65\\nTable 8: Label words and results on the SST-2\\ndataset with different strategies. \"+\" represents positive\\npolarity; \"-\" denotes negative polarity.\\n6.4 The inﬂuence of clues\\nAs mentioned in Section 3, clues are keywords,\\nphrases, contextual information, semantic meaning,\\nsemantic relationships, tones, references that\\nsupport making decisions. We remove different\\ntypes of words in clues and evaluate its inﬂuence\\non SST-2 and R8 datasets. Editing prompts\\nachieve this goal. The original prompt for\\nclue collecting is List CLUES (i.e., keywords,\\nphrases, contextual information, semantic meaning,\\nsemantic relationships, tones, references) that\\nsupport the sentiment determination of the input.\\nIf we want to remove keywords & phrases , we just\\nremove them from the prompt.\\n•w/o keywords & phrases : keywords and\\nphrases are surface evidence for making\\ndecisions such as \"like\" ,\"hate\" .\\n•w/o contextual information & semantic\\nmeaning : contextual information and\\nsemantic meaning are meaning in\\nsentences/paragraphs such as The author\\nexpress his happiness .\\n•w/o semantic relationships : semantic\\nrelationships refer to relations between\\nsubjects such as \"emotional danger\" suggests\\na romantic and thrilling relationship between\\nIdemoto and Kim that creates a positive\\nsentiment. .\\n•w/o tones : tones are the general mood of the\\ntext such as The sentence is expressed in an\\nobjective tone .\\n•w/o references : references are mentions of\\ncommonsense facts or books such as The\\nreference to the popular, comedic character\\n\"Ferris Bueller\" implies that the kid is seen in\\na positive light. .\\nExperimental results are shown in Table 9. For\\nR8 and SST-2 datasets, keywords play the key role\\nfor GPT predictions.', metadata={'source': './text_classification_with_llm.pdf', 'page': 11}),\n Document(page_content='Prompts SST-2 R8\\nClues 96.80 98.29\\nw/o keyword&phrase 96.21 96.91\\nw/o contextual info. 96.23 97.10\\nw/o semantic relations 96.30 97.38\\nw/o tones 96.40 97.35\\nw/o reference 96.50 97.19\\nTable 9: Label words and results on the SST-2 dataset\\nwith different strategies.\\nRanking SimCSE FT\\nCARP\\nRandom 95.39 95.99\\nHigh-to-Low 95.22 96.71\\nLow-to-High 96.39 96.80\\nTable 10: Accuracy scores on SST-2 when assembling\\ndemonstrations with different ranking strategies.\\n6.5 The effect of demonstration order\\nDuring experiments, we ﬁnd that the ranking\\norder of demonstration affect ﬁnal results. In this\\nsubsection, we further investigate the inﬂuence\\nof orders of demonstrations. As mentioned in\\nSection 3.3, we retrieved kdata instancesN=\\nfxj;yjgk\\nj=1according to the cosine similarity with\\nthe test sequence. Orders the demonstrations in the\\nprompt we investigate include:\\n•Random : randomly shufﬂe retrieved\\ndemonstrations.\\n•Low-to-High : demonstrations with lower\\nsimilarity scores come ﬁrst. Therefore\\ndemonstrations with higher similarity scores\\nare placed closer to the test sequence, which\\nis placed at the end of the prompt.\\n•High-to-Low : demonstrations with lower\\nsimilarity scores are placed closer to the test\\nsequence.\\nAs shown in Table 10, performance is sensitive\\nthe ordering of the demonstrations. The low-\\nto-high ordering achieves the best performance\\ncompared to random and high-to-low ordering.\\n6.6 Quality of the reasoning process\\nIn this paper, we use LLMs to generate rationable\\nexplanations instead of human editing. Therefore,\\nthe quality of generated reasoning process affects\\nthe ﬁnal results. In this subsection, we sample\\n500 training (text, clues, reason, label) pairs and\\nevaluate the generated reasoning process from the\\nfollowing perspectives:\\n(1) Reliability: Inspired by the emergentReliability(%) \"Fluency(ppl) #Logic Faithful(%) \"\\nSST-2 96.18 3.89 95.20\\nR8 95.34 3.29 94.55\\nTable 11: Results for evaluating the quality of\\ngenerated reasoning explanation. We sample 500 (text,\\nreason) instances for SST-2 and R8.\\ngeneralization ability of LLMs, we use zero-shot\\nGPT-3 (175B) as the self-critique model to evaluate\\nthe quality of generated reasoning processes. To\\nbe speciﬁc, we ask the GPT-3 to return yes/no if\\nthe generated reasoning process supports making\\ndecisions for the input text. If the GPT-3 returns\\n\"yes\", it denotes that the reasoning process is\\nreliable for making decisions. If the GPT-3 returns\\n\"no\", it represents that the reasoning process is not\\nreliable.\\nThe prompt for SST-2 is shown as follows:\\nIs the following REASONING process supporting\\ndeterminate sentiment label to INPUT? Please\\nanswer Yes or No.\\nINPUT: <text>\\nREASONING: <reasoning-process>\\nwhere <text> is the text sequence for the data\\nand<reasoning-process> is generated reasoning\\nprocess.\\n(2) Fluency: use LLMs to generate reasoning\\nexplanations is a reference-free text generation task.\\nWe use perplexity to evaluate the generated text.\\n(3) Logic Faithful: previous work often use\\nmodels, which are trained on natural language\\ninference datasets, to determine whether the given\\n“hypothesis” logically follows from the “premise”.\\nHowever, lacking annotation datasets, NLI-trained\\nmodels can not generalize across multiple domains\\n(e.g., opinion, reviews, news). Since then, we use\\n16-shot ICL with GPT-3 to evaluate whether the\\ngenerated rationable explanations can be entailed\\nfrom the input text. If the InstructGPT responds\\nwith \"entailment\", it denotes that the generated\\nreasoning process is logic faithful with the text.\\nOtherwise, it represents the reasoning process is not\\nfaithful to the text. We sample training instances\\nfrom the SNLI dataset (Bowman et al., 2015)\\nas demonstrations. And prompts are shown as\\nfollows:\\nGiven the premise and hypothesis, please justify\\nwhether the HYPOTHESIS can be entailed from\\nthe PREMISE. Please return yes or no.\\nPREMISE: <text>\\nHYPOTHESIS: <reasoning-process>', metadata={'source': './text_classification_with_llm.pdf', 'page': 12}),\n Document(page_content='Evaluation results are shown in Table 11. As\\ncan be seen, the reliability percentages for SST-\\n2 and R5 are higher than 95%. This indicates\\nthat it is feasible to use the model-generated\\nreasoning process as part of the prompts to augment\\nICL performances. The perplexity of generated\\nreasoning text is smaller than 4, which denotes that\\nthe generated reasoning text is ﬂuent. And scores\\nof logic faithful are larger than 93%, which is in\\nline with our expectation that LLMs can generate\\nreasonable explanations.\\n7 Conclusion\\nIn this paper, we introduce ClueAndReasoning\\nPrompting (CARP) for text classiﬁcation task.\\nCARP yields new SOTA performances on 4 out\\nof 5 widely-used text-classiﬁcation benchmarks.\\nMore importantly, we ﬁnd that CARP delivers\\nimpressive abilities on low-resource and domain-\\nadaption setups. In the future, we would like\\nto explore CARP on more natural language\\nunderstanding tasks.\\nReferences\\nSamuel R. Bowman, Gabor Angeli, Christopher\\nPotts, and Christopher D. Manning. 2015.\\nA large annotated corpus for learning natural\\nlanguage inference. In Proceedings of the 2015\\nConference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) . Association for\\nComputational Linguistics.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems , 33:1877–1901.\\nDuo Chai, Wei Wu, Qinghong Han, Fei Wu, and Jiwei\\nLi. 2020. Description based text classiﬁcation with\\nreinforcement learning. In International Conference\\non Machine Learning . PMLR.\\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020.\\nMixtext: Linguistically-informed interpolation of\\nhidden space for semi-supervised text classiﬁcation.\\narXiv preprint arXiv:2004.12239 .\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311 .Kevin Clark, Minh-Thang Luong, Quoc V Le, and\\nChristopher D Manning. 2020. Electra: Pre-\\ntraining text encoders as discriminators rather than\\ngenerators. arXiv preprint arXiv:2003.10555 .\\nAlexis Conneau, Holger Schwenk, Loïc Barrault,\\nand Yann Lecun. 2016. Very deep convolutional\\nnetworks for text classiﬁcation. arXiv preprint\\narXiv:1606.01781 .\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime\\nCarbonell, Quoc V Le, and Ruslan Salakhutdinov.\\n2019. Transformer-xl: Attentive language models\\nbeyond a ﬁxed-length context. arXiv preprint\\narXiv:1901.02860 .\\nRajarshi Das, Manzil Zaheer, Dung Thai, Ameya\\nGodbole, Ethan Perez, Jay-Yoon Lee, Lizhen Tan,\\nLazaros Polymenakos, and Andrew McCallum.\\n2021. Case-based reasoning for natural language\\nqueries over knowledge bases. arXiv preprint\\narXiv:2104.08762 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2018. Bert: Pre-training\\nof deep bidirectional transformers for language\\nunderstanding. arXiv preprint arXiv:1810.04805 .\\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,\\nXiaocheng Feng, Ming Gong, Linjun Shou, Bing\\nQin, Ting Liu, Daxin Jiang, et al. 2020. Codebert:\\nA pre-trained model for programming and natural\\nlanguages. arXiv preprint arXiv:2002.08155 .\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\\nSimcse: Simple contrastive learning of sentence\\nembeddings. arXiv preprint arXiv:2104.08821 .\\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu,\\nand Maosong Sun. 2021. Ptr: Prompt tuning\\nwith rules for text classiﬁcation. arXiv preprint\\narXiv:2105.11259 .\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\\nbert with disentangled attention. ArXiv .\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model ﬁne-tuning for text classiﬁcation.\\narXiv preprint arXiv:1801.06146 .\\nRadu Tudor Ionescu and Andrei M Butnaru. 2019.\\nVector of locally-aggregated word embeddings\\n(vlawe): A novel document-level representation.\\narXiv preprint arXiv:1902.08850 .\\nOzan Irsoy and Claire Cardie. 2014. Deep recursive\\nneural networks for compositionality in language.\\nAdvances in neural information processing systems ,\\n27.\\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\\nTomas Mikolov. 2016. Bag of tricks for efﬁcient text\\nclassiﬁcation. arXiv preprint arXiv:1607.01759 .', metadata={'source': './text_classification_with_llm.pdf', 'page': 13}),\n Document(page_content='Nitish Shirish Keskar, Bryan McCann, Lav R\\nVarshney, Caiming Xiong, and Richard Socher.\\n2019. Ctrl: A conditional transformer language\\nmodel for controllable generation. arXiv preprint\\narXiv:1909.05858 .\\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020. Nearest\\nneighbor machine translation. arXiv preprint\\narXiv:2010.00710 .\\nYoon Kim. 2014. Convolutional neural networks for\\nsentence classiﬁcation. In Conference on Empirical\\nMethods in Natural Language Processing .\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid,\\nYutaka Matsuo, and Yusuke Iwasawa. 2022. Large\\nlanguage models are zero-shot reasoners. ArXiv .\\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao.\\n2015. Recurrent convolutional neural networks for\\ntext classiﬁcation. In Proceedings of the AAAI\\nconference on artiﬁcial intelligence .\\nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY\\nChan, Kory Matthewson, Michael Henry Tessler,\\nAntonia Creswell, James L McClelland, Jane X\\nWang, and Felix Hill. 2022. Can language models\\nlearn from explanations in context? arXiv preprint\\narXiv:2204.02329 .\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\\nOptimizing continuous prompts for generation.\\narXiv preprint arXiv:2101.00190 .\\nYuxiao Lin, Yuxian Meng, Xiaofei Sun, Qinghong Han,\\nKun Kuang, Jiwei Li, and Fei Wu. 2021. Bertgcn:\\nTransductive text classiﬁcation by combining gcn\\nand bert. arXiv preprint arXiv:2105.05727 .\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\\nLawrence Carin, and Weizhu Chen. 2021. What\\nmakes good in-context examples for gpt- 3?arXiv\\npreprint arXiv:2101.06804 .\\nPengfei Liu, Xipeng Qiu, and Xuanjing Huang.\\n2016. Recurrent neural network for text\\nclassiﬁcation with multi-task learning. arXiv\\npreprint arXiv:1605.05101 .\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining\\napproach. arXiv preprint arXiv:1907.11692 .\\nAna Marasovi ´c, Iz Beltagy, Doug Downey, and\\nMatthew E Peters. 2021. Few-shot self-\\nrationalization with natural language prompts.\\narXiv preprint arXiv:2111.08284 .Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\\nCarroll L Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex\\nRay, et al. 2022. Training language models to follow\\ninstructions with human feedback. arXiv preprint\\narXiv:2203.02155 .\\nBo Pang and Lillian Lee. 2005. Seeing stars:\\nExploiting class relationships for sentiment\\ncategorization with respect to rating scales. arXiv\\npreprint cs/0506075 .\\nGuanghui Qin and Jason Eisner. 2021. Learning how\\nto ask: Querying lms with mixtures of soft prompts.\\narXiv preprint arXiv:2104.06599 .\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019a. Language\\nmodels are unsupervised multitask learners.\\nAlec Radford, Jeffrey Wu, Rewon Child, David\\nLuan, Dario Amodei, Ilya Sutskever, et al.\\n2019b. Language models are unsupervised\\nmultitask learners. OpenAI blog .\\nJack W Rae, Sebastian Borgeaud, Trevor Cai,\\nKatie Millican, Jordan Hoffmann, Francis Song,\\nJohn Aslanides, Sarah Henderson, Roman Ring,\\nSusannah Young, et al. 2021. Scaling language\\nmodels: Methods, analysis & insights from training\\ngopher. arXiv preprint arXiv:2112.11446 .\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the\\nlimits of transfer learning with a uniﬁed text-to-\\ntext transformer. The Journal of Machine Learning\\nResearch , 21(1):5485–5551.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-\\nbert: Sentence embeddings using siamese bert-\\nnetworks. arXiv preprint arXiv:1908.10084 .\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\\n2021. Learning to retrieve prompts for in-context\\nlearning. arXiv preprint arXiv:2112.08633 .\\nTimo Schick and Hinrich Schütze. 2020. Exploiting\\ncloze questions for few shot text classiﬁcation\\nand natural language inference. arXiv preprint\\narXiv:2001.07676 .\\nYeon Seonwoo, Guoyin Wang, Sajal Choudhary,\\nChangmin Seo, Jiwei Li, Xiang Li, Puyang Xu,\\nSunghyun Park, and Alice Oh. 2022. Ranking-\\nenhanced unsupervised sentence representation\\nlearning. arXiv preprint arXiv:2209.04333 .\\nWeijia Shi, Julian Michael, Suchin Gururangan, and\\nLuke Zettlemoyer. 2022. Nearest neighbor zero-\\nshot inference. arXiv preprint arXiv:2205.13792 .\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D Manning, Andrew Y Ng,\\nand Christopher Potts. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment', metadata={'source': './text_classification_with_llm.pdf', 'page': 14}),\n Document(page_content='treebank. In Proceedings of the 2013 conference on\\nempirical methods in natural language processing ,\\npages 1631–1642.\\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia\\nShi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari\\nOstendorf, Luke Zettlemoyer, Noah A Smith,\\net al. 2022. Selective annotation makes language\\nmodels better few-shot learners. arXiv preprint\\narXiv:2209.01975 .\\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\\n2019. How to ﬁne-tune bert for text classiﬁcation?\\nInChinese Computational Linguistics: 18th China\\nNational Conference, CCL 2019, Kunming, China,\\nOctober 18–20, 2019, Proceedings 18 . Springer.\\nXiaofei Sun, Yuxian Meng, Xiang Ao, Fei Wu, Tianwei\\nZhang, Jiwei Li, and Chun Fan. 2022. Sentence\\nsimilarity based on contexts. Transactions of the\\nAssociation for Computational Linguistics .\\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng,\\nHao Tian, Hua Wu, and Haifeng Wang. 2020.\\nErnie 2.0: A continual pre-training framework for\\nlanguage understanding. In Proceedings of the AAAI\\nconference on artiﬁcial intelligence , volume 34.\\nZijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng,\\nXiang Ao, Qing He, Fei Wu, and Jiwei Li.\\n2021. Chinesebert: Chinese pretraining enhanced\\nby glyph and pinyin information. arXiv preprint\\narXiv:2106.16038 .\\nJian Tang, Meng Qu, and Qiaozhu Mei. 2015.\\nPte: Predictive text embedding through large-scale\\nheterogeneous text networks. In Proceedings of\\nthe 21th ACM SIGKDD international conference on\\nknowledge discovery and data mining , pages 1165–\\n1174.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\\net al. 2022. Lamda: Language models for dialog\\napplications. arXiv preprint arXiv:2201.08239 .\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\\nKhot, and Ashish Sabharwal. 2022. Interleaving\\nretrieval with chain-of-thought reasoning for\\nknowledge-intensive multi-step questions. arXiv\\npreprint arXiv:2212.10509 .\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is\\nall you need. Advances in neural information\\nprocessing systems , 30.\\nZhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying\\nLiu, Haiyue Song, Jiwei Li, and Sadao Kurohashi.\\n2023. Gpt-re: In-context learning for relation\\nextraction using large language models. arXiv\\npreprint arXiv:2305.02105 .Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe\\nZhang, Dinghan Shen, Xinyuan Zhang, Ricardo\\nHenao, and Lawrence Carin. 2018. Joint embedding\\nof words and labels for text classiﬁcation. arXiv\\npreprint arXiv:1805.04174 .\\nShuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin\\nOuyang, Fei Wu, Tianwei Zhang, Jiwei Li, and\\nGuoyin Wang. 2023. Gpt-ner: Named entity\\nrecognition via large language models. arXiv\\npreprint arXiv:2304.10428 .\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\\n2022a. Emergent abilities of large language models.\\narXiv preprint arXiv:2206.07682 .\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. arXiv preprint arXiv:2201.11903 .\\nJason Wei and Kai Zou. 2019. Eda: Easy data\\naugmentation techniques for boosting performance\\non text classiﬁcation tasks. arXiv preprint\\narXiv:1901.11196 .\\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,\\nMark Riedl, and Yejin Choi. 2021. Reframing\\nhuman-ai collaboration for generating free-text\\nexplanations. arXiv preprint arXiv:2112.08674 .\\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang\\nLuong, and Quoc Le. 2020. Unsupervised data\\naugmentation for consistency training. Advances in\\nneural information processing systems .\\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\\nand Tengyu Ma. 2021. An explanation of in-context\\nlearning as implicit bayesian inference. arXiv\\npreprint arXiv:2111.02080 .\\nLinting Xue, Noah Constant, Adam Roberts, Mihir\\nKale, Rami Al-Rfou, Aditya Siddhant, Aditya\\nBarua, and Colin Raffel. 2020. mt5: A massively\\nmultilingual pre-trained text-to-text transformer.\\narXiv preprint arXiv:2010.11934 .\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime\\nCarbonell, Russ R Salakhutdinov, and Quoc V Le.\\n2019. Xlnet: Generalized autoregressive pretraining\\nfor language understanding. Advances in neural\\ninformation processing systems , 32.\\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\\nAlex Smola, and Eduard Hovy. 2016. Hierarchical\\nattention networks for document classiﬁcation.\\nInProceedings of the 2016 conference of the\\nNorth American chapter of the association for\\ncomputational linguistics: human language\\ntechnologies , pages 1480–1489.\\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\\nGraph convolutional networks for text classiﬁcation.\\nInProceedings of the AAAI conference on artiﬁcial\\nintelligence , volume 33, pages 7370–7377.', metadata={'source': './text_classification_with_llm.pdf', 'page': 15}),\n Document(page_content='Xi Ye and Greg Durrett. 2022. The unreliability\\nof explanations in few-shot prompting for textual\\nreasoning. Advances in neural information\\nprocessing systems .\\nFang Zeng, Niannian Chen, Dan Yang, and Zhigang\\nMeng. 2022. Simpliﬁed-boosting ensemble\\nconvolutional network for text classiﬁcation.\\nNeural Process. Lett. , 54(6).\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher\\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\\n2022a. Opt: Open pre-trained transformer language\\nmodels. arXiv preprint arXiv:2205.01068 .\\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\\nCharacter-level convolutional networks for text\\nclassiﬁcation. Advances in neural information\\nprocessing systems , 28.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\\nSmola. 2022b. Automatic chain of thought\\nprompting in large language models. arXiv preprint\\narXiv:2210.03493 .\\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\\nFactual probing is [mask]: Learning vs. learning to\\nrecall. arXiv preprint arXiv:2104.05240 .\\nA Dataset\\nSST-2 (Socher et al., 2013), R8, R529,\\nAGNews (Zhang et al., 2015) and MR (Movie\\nReview) (Pang and Lee, 2005).\\n•SST-2 : The original data in SST-2 are\\nsampled from snippets of Rotten Tomatoes\\nHTML ﬁles. We use the same train/dev/test\\nsplits with Socher et al. (2013).\\n•R8 and R52 : R8 and R5211 are two\\nsubsections of the Reuters collection,\\ncontaining 8 and 52 classiﬁcations,\\nrespectively. The R8 dataset is composed\\nof 5,485 documents for training and 2,189\\ndocuments for testing. The R52 dataset is\\ncomposed of 6,532 training and 2,568 test\\ndocuments.\\n•AGNews : The AG News consists of news\\narticles from the AG’s corpus. The dataset\\ncontains 30,000 training and 1,900 testing\\nexamples for each class.\\n•MR (Movie Review) : The MR contains\\nreviews of ﬁlms for determining whether a\\nsentiment is either positive or negative. The\\ncorpus has 10,662 reviews. We follow (Tang\\net al., 2015) and use the same train/test split.\\n9R8 and R52 are from https://www.cs.umb.edu/\\n~smimarog/textmining/datasets/Dataset Task # Label Source # Train # Dev # Test\\nSST-2 sentiment 2 review 6,920 872 1,821\\nAGNews topic 4 news 96,000 24,000 7,600\\nR8 topic 8 news 4,941 544 2,189\\nR52 topic 52 news 5,905 627 2,568\\nMR sentiment 2 reviews 6,398 710 3,554\\nTable 12: Benchmark Dataset\\nDataset Task # Label Source # Train # Dev # Subtest\\nSST-2 sentiment 2 review 6,920 872 728\\nAGNews topic 4 news 96,000 24,000 760\\nR8 topic 8 news 4,941 544 875\\nR52 topic 52 news 5,905 627 1,027\\nMR sentiment 2 reviews 6,398 710 888\\nTable 13: Dataset Subsets\\nB Hyper-parameters\\nB.1 Fine-tuning Hyper-parameters\\nWe ﬁne-tune RoBERTa and RoBERT-GCN on\\n4 NVIDIA 3090 GPUs with FP16. Model\\nhyper-parameters are tuned on the validation set,\\nwhere learning rate f2e-5, 3e-5, 4e-5g, batch size\\nf16;32;32g, a dropout rate of 0.3, a weight decay\\nof 0.01, a warmup proportion of 0.01.\\nB.2 The inﬂuence of hyper-parameters\\nWe investigate the effect of model hyper-\\nparameters including temperature, frequency\\npenalty. We conduct experiments with Instruct-\\nGPT3 on the SST-2 dataset.\\nTemperature The temperature \\x1ccontrols the\\ngenerated text variety when another hyper-\\nparametertopp=1. More higher \\x1c, more variety\\nis introduced. When \\x1cis close to 0, the model\\ngenerates the same result with the greedy decoding\\nmethod. To exploit the effect of temperature \\x1c,\\nwe set\\x1cfrom 0 to 1.0. Experimental results are\\nshown in Table B.2. We tokenize the response text\\nwith GPT-Tokenizer10and then count the number\\nof tokens.\\n\\x1c SST-2 Accuracy\\n\\x1c= 0:0 96.39\\n\\x1c= 0:2 96.48\\n\\x1c= 0:4 96.40\\n\\x1c= 0:6 96.59\\n\\x1c= 0:8 96.68\\n\\x1c= 1:0 96.70\\n10https://platform.openai.com/tokenizer', metadata={'source': './text_classification_with_llm.pdf', 'page': 16}),\n Document(page_content='SST-2 : positive/negative sentiment analysis\\nLabel Word Map {0: Negative, 1: Positive}\\nZero-Shot\\nClassify Prompt: Please classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nSENTIMENT:\\nReason-Classify Prompts: Please classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nFindclue-Reason-Classify Step 1:\\nPlease classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nStep 2:\\nPlease classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nCLUES: <step-1-response>\\nFew-Shot\\nClassify Prompt: Please classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <demo-sent>\\nSENTIMENT: <demo-label-word>\\nINPUT: <demo-sent>\\nSENTIMENT: <demo-label-word>\\nINPUT: <sent>\\nSENTIMENT:\\nReason-Classify Prompts: Step 1:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nStep 2:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <test-sent>\\nFindclue-Reason-Classify Prompts: Step 1:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nStep 2:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <test-sent>\\nTable 14: Examples of prompts for setups in Section 3.', metadata={'source': './text_classification_with_llm.pdf', 'page': 17}),\n Document(page_content='R8 : topic classiﬁcation\\nLabel Word Map {0: Money/Foreign Exchange, 1: Acquisitions, 2: Trade, 3: Interest Rates,\\n4: Shipping, 5: Earnings and Earnings Forecasts, 6: Grain, 7: Crude Oil}\\nZero-Shot\\nClassify Prompt: Please classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nSENTIMENT:\\nReason-Classify Prompts: Please classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nFindclue-Reason-Classify Step 1:\\nPlease classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nStep 2:\\nPlease classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nCLUES: <step-1-response>\\nFew-Shot\\nClassify Prompt: Please classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <demo-sent>\\nSENTIMENT: <demo-label-word>\\nINPUT: <demo-sent>\\nSENTIMENT: <demo-label-word>\\nINPUT: <sent>\\nSENTIMENT:\\nReason-Classify Prompts: Step 1:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nStep 2:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <test-sent>\\nFindclue-Reason-Classify Prompts: Step 1:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nStep 2:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <test-sent>\\nTable 15: Examples of prompts for setups in Section 3.', metadata={'source': './text_classification_with_llm.pdf', 'page': 18}),\n Document(page_content='MR : topic classiﬁcation\\nLabel Word Map {0: Negative, 1: Positive}\\nZero-Shot\\nClassify Prompt: Please classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nSENTIMENT:\\nReason-Classify Prompts: Please classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nFindclue-Reason-Classify Step 1:\\nPlease classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nStep 2:\\nPlease classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <sent>\\nCLUES: <step-1-response>\\nFew-Shot\\nClassify Prompt: Please classify the overall SENTIMENT polarity of the INPUT sentence as Positive or Negative.\\nINPUT: <demo-sent>\\nSENTIMENT: <demo-label-word>\\nINPUT: <demo-sent>\\nSENTIMENT: <demo-label-word>\\nINPUT: <sent>\\nSENTIMENT:\\nReason-Classify Prompts: Step 1:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nStep 2:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <test-sent>\\nFindclue-Reason-Classify Prompts: Step 1:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nStep 2:\\nClassify the sentiment of the input sentence as positive or negative.\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <demo-sent>\\nREASONING: <step-1-generated>\\nSENTIMENT: <demo-label-word>\\nINPUT: <test-sent>\\nTable 16: Examples of prompts for setups in Section 3.', metadata={'source': './text_classification_with_llm.pdf', 'page': 19}),\n Document(page_content='Training language models to follow instructions\\nwith human feedback\\nLong Ouyang∗\\nJeff Wu∗\\nXu Jiang∗\\nDiogo Almeida∗\\nCarroll L. Wainwright∗\\nPamela Mishkin∗\\nChong Zhang\\nSandhini Agarwal\\nKatarina Slama\\nAlex Ray\\nJohn Schulman\\nJacob Hilton\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\na user’s intent. For example, large language models can generate outputs that\\nare untruthful, toxic, or simply not helpful to the user. In other words, these\\nmodels are not aligned with their users. In this paper, we show an avenue for\\naligning language models with user intent on a wide range of tasks by ﬁne-tuning\\nwith human feedback. Starting with a set of labeler-written prompts and prompts\\nsubmitted through the OpenAI API, we collect a dataset of labeler demonstrations\\nof the desired model behavior, which we use to ﬁne-tune GPT-3 using supervised\\nlearning. We then collect a dataset of rankings of model outputs, which we use to\\nfurther ﬁne-tune this supervised model using reinforcement learning from human\\nfeedback. We call the resulting models InstructGPT. In human evaluations on\\nour prompt distribution, outputs from the 1.3B parameter InstructGPT model are\\npreferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.\\nMoreover, InstructGPT models show improvements in truthfulness and reductions\\nin toxic output generation while having minimal performance regressions on public\\nNLP datasets. Even though InstructGPT still makes simple mistakes, our results\\nshow that ﬁne-tuning with human feedback is a promising direction for aligning\\nlanguage models with human intent.\\n1\\nIntroduction\\nLarge language models (LMs) can be “prompted” to perform a range of natural language process-\\ning (NLP) tasks, given some examples of the task as input. However, these models often express\\nunintended behaviors such as making up facts, generating biased or toxic text, or simply not following\\nuser instructions (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al.,\\n2021; Tamkin et al., 2021; Gehman et al., 2020). This is because the language modeling objective\\n∗Primary authors. This was a joint project of the OpenAI Alignment team. RL and JL are the team leads.\\nCorresponding author: lowe@openai.com.\\n†Work done while at OpenAI. Current afﬁliations: AA: Anthropic; PC: Alignment Research Center.\\narXiv:2203.02155v1  [cs.CL]  4 Mar 2022\\n1.3B\\n6B\\n175B\\nModel size\\n0.2\\n0.4\\n0.6\\nWin rate against SFT 175B\\nModel\\nPPO-ptx\\nPPO\\nSFT\\nGPT (prompted)\\nGPT\\nFigure 1: Human evaluations of various models on our API prompt distribution, evaluated by how\\noften outputs from each model were preferred to those from the 175B SFT model. Our InstructGPT\\nmodels (PPO-ptx) as well as its variant trained without pretraining mix (PPO) signiﬁcantly outperform\\nthe GPT-3 baselines (GPT, GPT prompted); outputs from our 1.3B PPO-ptx model are preferred to\\nthose from the 175B GPT-3. Error bars throughout the paper are 95% conﬁdence intervals.\\nused for many recent large LMs—predicting the next token on a webpage from the internet—is\\ndifferent from the objective “follow the user’s instructions helpfully and safely” (Radford et al., 2019;\\nBrown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Thus, we say that\\nthe language modeling objective is misaligned. Averting these unintended behaviors is especially\\nimportant for language models that are deployed and used in hundreds of applications.\\nWe make progress on aligning language models by training them to act in accordance with the user’s\\nintention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions\\nand implicit intentions such as staying truthful, and not being biased, toxic, or otherwise harmful.\\nUsing the language of Askell et al. (2021), we want language models to be helpful (they should\\nhelp the use', metadata={'Published': '2022-03-04', 'Title': 'Training language models to follow instructions with human feedback', 'Authors': 'Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe', 'Summary': \"Making language models bigger does not inherently make them better at\\nfollowing a user's intent. For example, large language models can generate\\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\\nwords, these models are not aligned with their users. In this paper, we show an\\navenue for aligning language models with user intent on a wide range of tasks\\nby fine-tuning with human feedback. Starting with a set of labeler-written\\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\\noutputs, which we use to further fine-tune this supervised model using\\nreinforcement learning from human feedback. We call the resulting models\\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\\nimprovements in truthfulness and reductions in toxic output generation while\\nhaving minimal performance regressions on public NLP datasets. Even though\\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\\nhuman feedback is a promising direction for aligning language models with human\\nintent.\"})]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [item for sublist in documents for item in sublist]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:41:22.002072600Z",
     "start_time": "2023-05-24T20:41:21.662900400Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(llm, chain_type=\"refine\", question_prompt=PROMPT, refine_prompt=refine_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:49:52.802283500Z",
     "start_time": "2023-05-24T20:41:21.780539500Z"
    }
   },
   "outputs": [],
   "source": [
    "res = summary_chain({\"input_documents\": documents}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'output_text': '\\nThis paper introduces Clue And Reasoning Prompting (CARP), a framework for text classification via large language models. CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification. CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for kNN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM’s generalization ability and the task-specific evidence provided by the full labeled dataset. CARP also uses a demonstration sampling strategy that includes random sampling and kNN sampling, which retrieves semantically similar examples. Additionally, CARP uses a progressive reasoning strategy that involves clue collection, reasoning, and decision making, which mimics how humans make decisions. CARP yields new SOTA performances on 4 out of 5 widely-used text-classification benchmarks and impressive abilities on low-resource and domain-adaptation setups, such as 16 examples per class to supervised models trained on the full training set containing more than 1 thousand.'}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T20:49:52.813905400Z",
     "start_time": "2023-05-24T20:49:47.654594600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:50:26.122397900Z",
     "start_time": "2023-05-24T20:50:25.962912400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nThis paper introduces Clue And Reasoning Prompting (CARP), a framework for text classification via large language models. CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification. CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for kNN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM’s generalization ability and the task-specific evidence provided by the full labeled dataset. CARP also uses a demonstration sampling strategy that includes random sampling and kNN sampling, which retrieves semantically similar examples. Additionally, CARP uses a progressive reasoning strategy that involves clue collection, reasoning, and decision making, which mimics how humans make decisions. CARP yields new SOTA performances on 4 out of 5 widely-used text-classification benchmarks and impressive abilities on low-resource and domain-adaptation setups, such as 16 examples per class to supervised models trained on the full training set containing more than 1 thousand.'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['output_text']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
